{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b79b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intro to svms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e566d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression is a linear classifier with a logistic loss function\n",
    "# linear svms are also linear classifiers but they use the hinge loss function (l2 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e32cee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hinge loss function is mostly different from logistic loss function in the sense that once the raw model output is greater than one, the hinge loss function is completely flat\n",
    "# this just means we predicted a sample correctly with a certain margin of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bda50b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# support vectors are examples that are NOT In the flat part of the loss diagram\n",
    "# in other words, they are examples that are incorrectly predicted OR that are really close to the decision boundary\n",
    "# the first case is indicative of a large loss function, whereas the second case is indicative of a failure to meet a certain margin of error in terms of probability\n",
    "# the amount of regularization controls how close is too close\n",
    "# support vectors are the examples that matter to your fit\n",
    "# if an example is not a support vector, removing it has no effect on the model, because the loss function is zero\n",
    "# as opposed to logistic regression, where all examples matter to the fit, in svm, only some samples matter to the fit\n",
    "# svms are popular because they are fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6dbfb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some people talk about svms in terms of \"maximizing margin\", which is just maximizing the distance between support vectors and the decision boundary\n",
    "# in other words, it's trying to make the distinction between each class as stark as possible\n",
    "# if a dataset is linearly separable (so 100% training accuracy) and regularization is small, svm will maximize the margin\n",
    "# however, this is rarely the case, so we don't really worry about maximizing the margin\n",
    "# max margin theory does extend towards non-separable datasets tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef5606ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27aba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear SVM\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X, y)\n",
    "plot_classifier(X, y, svm, lims=(11,15,0,6))\n",
    "\n",
    "# Make a new data set keeping only the support vectors\n",
    "print(\"Number of original examples\", len(X))\n",
    "print(\"Number of support vectors\", len(svm.support_))\n",
    "X_small = X[svm.support_]\n",
    "y_small = y[svm.support_]\n",
    "\n",
    "# Train a new SVM using only the support vectors\n",
    "svm_small = SVC(kernel=\"linear\")\n",
    "svm_small.fit(X_small, y_small)\n",
    "plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
