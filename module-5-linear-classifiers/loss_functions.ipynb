{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riddlemeS4m/machine-learning-scientist-datacamp/blob/dev-google/loss_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Insert Title Here]"
      ],
      "metadata": {
        "id": "pxSNd4juMrAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "rH_8y2UVMp46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GDRzdbhNEpd",
        "outputId": "fde61f26-ace7-4e67-808d-c3f70826ff64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# linear classifiers predict categories based on a decision boundary\n",
        "# logistic regression and svm are types of linear classifiers, both aim to draw a straight line to separate classes in training data (just using different methods)\n",
        "# nonlinear classifiers are needed when data is not linearly separable"
      ],
      "metadata": {
        "id": "r7ldTYrqNLvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(3)\n",
        "y = np.arange(3,6)\n",
        "\n",
        "X*y\n",
        "np.sum(x*y)\n",
        "# same as vv\n",
        "x@y\n",
        "\n",
        "# ^^ that's a dot product"
      ],
      "metadata": {
        "id": "iIyhUR8tDnjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict function for linear classifiers vv\n",
        "# raw model output = coefficients * features + intercept\n",
        "# compute output then check the sign: if positive, predict one, if negative, predict the other\n",
        "# logistic regression and svm have the same predict function, it's just that their fit functions are different"
      ],
      "metadata": {
        "id": "yloVDC6bEbMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "lr = LogisticRegression()\n",
        "svm = SVC()\n",
        "\n",
        "lr.fit(X, y)\n",
        "lr.predict(X)[10]\n",
        "lr.predict(X)[20]"
      ],
      "metadata": {
        "id": "7nE_b3iVFK3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.coef_ @ X[10] + lr.intercept_ # raw model output\n",
        "lr.coef_ @ X[20] + lr.intercept_"
      ],
      "metadata": {
        "id": "hwzxeFnYFn1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the sign of the raw model output tells you what side of the decision boundary you're on"
      ],
      "metadata": {
        "id": "xJ_BgN_CFx-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss functions vv"
      ],
      "metadata": {
        "id": "Qi6uHcbhG7DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function is a penalty score that tells us how poorly the model is doing\n",
        "# minimizing the loss function is jiggling the parameters until it's as small as possible\n",
        "# the fit function is basically running the minimizing the loss function\n",
        "# model.score() isn't usually the same as the loss function value"
      ],
      "metadata": {
        "id": "wshqqn5dG9Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# can't use least squares as a loss function for classification problems, because output is categorical (not numeric)\n",
        "# might be tempted to use number of errors (0 being not an error, 1 being an error) as a loss function\n",
        "# however, logistic regression and svc don't use this because it's hard to minimize (not sure why, probably computational reasons)"
      ],
      "metadata": {
        "id": "EtQE2jnxHfF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "minimize(np.square, 0).x\n",
        "minimize(np.square, 2).x"
      ],
      "metadata": {
        "id": "MGA2POuUH_kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise"
      ],
      "metadata": {
        "id": "25V-oFmTJBG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The squared error, summed over training examples\n",
        "def my_loss(w):\n",
        "    s = 0\n",
        "    for i in range(y.size):\n",
        "        # Get the true and predicted target values for example 'i'\n",
        "        y_i_true = y[i]\n",
        "        y_i_pred = w@X[i]\n",
        "        s = s + (y_i_true - y_i_pred)**2\n",
        "    return s\n",
        "\n",
        "# Returns the w that makes my_loss(w) smallest\n",
        "w_fit = minimize(my_loss, X[0]).x\n",
        "print(w_fit)\n",
        "\n",
        "# Compare with scikit-learn's LinearRegression coefficients\n",
        "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
        "print(lr.coef_)"
      ],
      "metadata": {
        "id": "T2qFnGmPJBnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function diagrams vv"
      ],
      "metadata": {
        "id": "q7ikYq5IPK3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the idea is, for a classifier model, you need a loss function that is high for very incorrect predictions, and is 0 for correct predictions\n",
        "# the least squares loss function doesn't work for classifier models, because it's loss function is a quadratic function\n",
        "# instead, logistic regression and svm use different loss functions\n",
        "# two of them are called logistic and hinge loss functions"
      ],
      "metadata": {
        "id": "QsmWkxELPN-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise"
      ],
      "metadata": {
        "id": "zrUtJdCxZDnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mathematical functions for logistic and hinge losses\n",
        "def log_loss(raw_model_output):\n",
        "   return np.log(1+np.exp(-raw_model_output))\n",
        "def hinge_loss(raw_model_output):\n",
        "   return np.maximum(0,1-raw_model_output)\n",
        "\n",
        "# Create a grid of values and plot\n",
        "grid = np.linspace(-2,2,1000)\n",
        "plt.plot(grid, log_loss(grid), label='logistic')\n",
        "plt.plot(grid, hinge_loss(grid), label='hinge')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "THkRYzwIZC9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise"
      ],
      "metadata": {
        "id": "Rix1eGv3bHcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The logistic loss, summed over training examples\n",
        "def my_loss(w):\n",
        "    s = 0\n",
        "    for i in range(0, len(X)):\n",
        "        raw_model_output = w@X[i]\n",
        "        s = s + log_loss(raw_model_output * y[i])\n",
        "    return s\n",
        "\n",
        "# Returns the w that makes my_loss(w) smallest\n",
        "w_fit = minimize(my_loss, X[0]).x\n",
        "print(w_fit)\n",
        "\n",
        "# Compare with scikit-learn's LogisticRegression\n",
        "lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)\n",
        "print(lr.coef_)"
      ],
      "metadata": {
        "id": "N47ct657bH36"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
